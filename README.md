# Mapping-Moral-Reasoning-in-LLMs

This repository hosts a comprehensive analytical framework and accompanying code for profiling moral reasoning and safety prioritization in large language models. It includes:

### ollama_scenario_runner.py: Script to query multiple LLMs across structured ethical scenarios and collect responses.

### enhanced_ethics_analyzer.py: Python class for multi-dimensional ethical analysis, integrating dictionary-based and semantic methods.

### quantitative_pipeline.py: End-to-end quantitative analysis pipeline computing metrics (entropy, reasoning complexity, safety emphasis), statistical tests, and PCA/clustering.

### robustness_ablation.py: Robustness and ablation study script for validating taxonomy stability using Adjusted Rand Index and HDBSCAN.

Use these scripts to reproduce the study, generate new data, and perform in-depth moral reasoning analyses of LLM outputs. For detailed instructions, please refer to the code comments and inline documentation within each script.
